 Welcome, everyone, to the biweekly product updates. I'm going to start with a couple announcements real quick before we hand it off to demos and discussions products. I don't know why I have to explicitly let Tristan in him. He must be in China today. Calling in from China, Alexis is going to block that soon and Tristan just so you know. Sorry, back back to more announcement things. Josh has moved the delivery system office hours that happen every Friday to be more of a true like office hour style, like the k8s tool and the manifests tool run and obviously people saw the. The blessing events calendar being added in addition to staff adventures and separating the intense those two calendars you'll find the product office hours on the blessing. So that'll be for people to show up and you know bring things they want to talk about the team will definitely be more in like receive mode and discussion about any, you know, ins and outs or decisions or issues in the backlog, whatever needs to be. So this will still happen Friday very very optional hopefully people still are still interested in come talk about stuff, but this one will then be transition to be more of like expressing what product is up to very intentionally what we've been creating and kind of where we're heading. So you'll hear you know facilitate that you'll hear from different you know tech leads or people from the different teams so it's to include delivery system will kind of wrap in delivery system with the open source projects to talk about product things here. So for today I put out the announcements channel Jared's going to talk some of the speech to tech stuff that the leak broad team was doing for kind of an urgent request that came in that's pretty interesting will start there. Brandt now has a fully functioning team growing team on the Lewis side and some connections to P1 and some some code and refactoring that started which is pretty exciting so he's got some updates. Case does as well on the the manifests tool side now that he's taken that over from Jeff and then if Jeff is on I haven't checked the or up to 60 some people which is great doing some delivery system core rewriting and I think that we're going to kind of move forward with some pretty big breaking changes there's Jeff will kind of demo what we've done so far and talk about where we're headed over the next couple weeks. So it should be a lot of information probably will take all the 60 minutes today and I'll only take this this five to open things up so Jared over to you. Awesome thank you i'm going to go I'm going to do really quickly just a run through of all the activities that we've done because there's too many things that we. There to just demo through. So I'm going to pull this up real quick just kind of run through kind of our October activities going into November the the AI tool AI team is is under a pretty tight deadline right now in the sense that we really want to ship out AI for national security coming. At least internally very soon and and start to expand that that's go but. So as far as October goes beyond board of two new engineers got to Harvey came over to us from working on CMS and Justin law joined us. After just departing with the his activity role the Air Force so so welcome Justin and welcome goto. This one would come back to at the end i'll I'll revisit that once I get to the rest of this but we added in this is all in conjunction with working with partners both who have urgent needs as well as on top of our our normal product demands so right now the the AI tool AI team is is working very actively in multiple contracts as well as developing an open source tool as well as developing a product in responding to. To national security events all the same time so lots of hard work going on by the team but. As part of that we added a an embeddings back end that we plan on using for AI for national security so what I want to break down embeddings real quick is is this is the core engine that allows us to. Perform one of a core tasks of and for national security when we launch and that's going to be retrieval augment a generation and what that means is we now have a tooling that can say that can cluster things together like i know that a cat in a kit and are the same but I know they cluster separately from like baby and adult and so. So being able to understand the semantic similarity across a wide range of content is what having an embeddings back end is all about and this one is the best in class that's out there right now scoring we're still doing evaluations on it but this is supporting being able to perform retrieval augment a generation and secret and top secret environments. We extracted a very fast and hard for back end as well as as made a candidate for one of the. The prime LM's we're going to be using and you'll see that today in a demo as we do some summarization. And we're evaluating a couple different fine tuneings of that Cynthia Samantha and Mr. on struct. We are looking the reason these are all different is that you have the base foundational model like it's trained and then these are how they get trained into activities like chat like code like instructions other things and so we're evaluating for are the various AI for national security. Features we want to support which flavor of mistro we need. One thing to know about mistro is it is the first fully open model that we're aware of that is high quality that we can use without commercial writers that we can use without acceptable use policies and it is outbeating models that are 13 billion to 34 billion in size much larger and there's reasons for that if you want the technical details feel free to talk to me after it. But it's a very, very impressive model made by people who came out of open AI and a lot of these companies wanting to create a truly open solution. We update our whisper back end for those who don't know whisper and there's activities going on at the beauty right now that involve whisper this is a speech text model so if I'm speaking this is how this will be transcribed in fact will transcribed this meeting after using whisper with lead frog. But what whisper does is provides that ability to transcription and translation across 98 languages of which about 50 are good enough from award error rate sport to be considered production ready. We have that support for transcripts summarization talk about that in a minute we shipped at least an initial spell chat UI interface and by the way that may be that I forgot Greg Greg Clark welcome to the chat. He was well helping us design and build out that interface and an experience make that better. We also did our first electric boat and blue forge alliance release for them and are kicking that off. Back to the chain guard derived images so we are we are shipping very minimal hardened images with chain guard derived images we're not using chain guards images anymore. We are maintaining that on our own I put a link up for product techniques to look at but I will socialize that more internally right now we have Python 3 to 11 and 3.11 Dev if anyone needs something that repose up and we can talk about how we start to expand the use of that other activities we've got going on. So we're supporting an urgent and critical national security demand from the DOD that involves transcription of content as well as summarization of that content. We're on the trade ones marketplace now is a fundable solution thanks and current congrats to Baron for working on that video and getting that across the line. We're working with a partner for deployment of those embedding the high side environments like I said and we also met with Nvidia government on getting part in CUDA images because that is that is a bit of a holy grail in this industry of getting hearted images that are supportive of GPUs but also supporting their triton back end and this is in in support of another high side cyber command customer. And then upcoming work we've got going on is working on an evaluation framework for iterating towards AI for national security. This means that as we're making changes to our prompts and I'll show a little bit of code for this for our retrieval augmented generation or as we're changing models. We just want to know for we're doing better in in getting questions or responses were so the same. So we what we we just need some sort of metrics and we're when you're dealing with something that is not deterministic in nature. It can be a little hard to get a just a basic test driven development environment up. You're really looking at scores and so we're beginning the ADR for that scoring process. We're going to continue on basically the AI for national security work and that's going to include that that chat AI and then just production observability. And then just our normal chopping wooden carrying water activities, which is like automation dependency updates. Depending on dependency updates may sound a little rote, but in this world. For example, an update to Nvidia GPU drivers came out yesterday that was a 20% increase in inference speed. So a dependency for us can mean a massive increase in the amount of prints we can hit in that mission environments. So we track those ones that are important and update towards those. So let's switch switch over to some code real quick in a demo as well as look at any questions if they came in. No questions great. Okay, so let's go look at an example performing summarization. So this is coming out from a meeting here at at different staffs. We can ignore some of the ugly code here, but I want to talk a little bit about methodology because. It's not like you can go dump a one hour meeting into even chat GPC today and get a useful functional summary out of it. You blow out the size of that model and what we're doing is we're trying to make this be able to run down to a CPUs like a CPU based laptop if we had to. So we're trying to shrink a lockdown into performing transcription and summarization into a small little window. So we're doing a summer like we're doing summarization and batches. So if I were to run this. And you know, I'm actually going to switch swap over to my terminal. What we're going to see here. Is. A bunch of errors, of course, but what we're going to see for real here is I've already done this transcription the transcription takes about three minutes to do on the hardware that I'm on at the moment. And so once the transcription is done, we can begin taking that in batches and begin to summarize those batches. So this was a one hour meeting that happened at company and we can take that in about 14 chunks for for one hour meeting and begin to break out summaries of those individual chunks and at the end refine that into a larger full summary. Now you can imagine if you are attempting to transcribe and summarize meetings in a very critical hotspot environments fastest possible. This takes up a lot of people's time to go listen to an hour of audio audio hour video and draft up those summaries. In fact, it's taking up the full time of many people in this use case. And so whereas this isn't a perfect option, it does create about a 93% compression of information from the original transcript down into the summaries of what this is about. And this meeting in particular is Jeff approved. So he looked at the summaries that like, yeah, that's basically the gist of what happened and maybe some things get lost along the way, but in a minute, we're able to to save an hour of someone's time in going and synthesizing it. Leifrog AI is very much about human in the loop. So, so, and I cut out the refine step here because it takes a little bit longer. It's a little less predictable at the moment. But, you know, that's some errors here. We've got Lulu. We've got leaf log, right? We there, there are some minor issues because it just doesn't have the context for that. But we do get a pretty accurate summary of what happened in that meeting. And we look to roll us out even internally for use and test against other meetings. We updated Doug translate to be there. And so we want to make sure that this is available internally as well for our own meetings. Yes, Lucas asked a great question, spoken language to target language. Whisper is trained to output English always. You would use another LM to retranslate it back, but it can take input languages from all 93 languages that's been trained on. So that's all I have for this particular product update. You know, there will be more UIs coming down the road and I'll let everyone know when Doug translate is up. But, you know, we were saving the DOD time and energy across the entire gamut at the moment. Any questions for Jared? As a, you know, we have several people at our company who have been on the operational side of the military. It's kind of crazy to think about like there's a for those that don't know people that process intelligence as a full time job meeting. They take information, whether it be text audio video image, whatever from all over the world and try to synthesize whatever the hell is going on. So they form somebody to make decisions and those decisions are really, really important and very time critical. Just an example of an hour of video that it gets summarized in three minutes, right, is pretty impressive. And you got to think about how this extrapolates to thousands and thousands of hours of like anything really is an intelligence source in this case audio. But there's all sorts of applications for this. And I think they're super cool. I see we got several hands up now. So I'll stop talking. I don't know who is first. But I don't think you reversed. I can go. I would just like to point out, I think Jared, you and I did this manually for a meeting a couple months ago. I had come to Jared and said, hey, I'm getting all these transcripts from a Google and they're basically useless. It would be really great if we could, you know, since we record all of our, you know, things in Google and we get these transcripts if we could summarize it. I bring this up to say, as you're going through your day, if you think fine things were like, hey, we're collecting this data and it's practically useless because we just don't have the brain power to go through and summarize or compute, like, please reach out to Jared, like if there's if there's data we're collecting and just not using, right, like find a way to and maybe you don't know how I think that's the other thing, like I didn't even know if it was possible or how to do it. I came to Jared and I said, hey, I tried to throw this entire transcript, it was like 18 pages and attack you be T and chat you be T can't summarize it for me. And he goes, oh, well, and then he was able to come through and figure out, hey, if we break it into chunks, we summarize those chunks and we have it summarize those chunks. Anyway, I wanted to share the story there because I think we're going to find really great use cases, even within the company just by thinking about those moments where I have all this data. I don't have the time or I don't have the ability to go through and synthesize it myself. So yeah, I just wanted to share that story behind it and encourage you guys to reach out if you ever find yourself in that situation. The team can help you figure out how to do it because I had no idea. Also a good time to advertise that that is most likely going to be a Dashday topic around how AI can help us with knowledge management or even just AI around company processes. And so I'm still I think working on dates and solidifying topics, but if what Madeline talked about interest you and you want to use something you use the AI tool or something like it to improve a company process or even just knowledge management in general. Be a good opportunity to focus on that for a couple days. Lucas. Yeah, I just wanted to pile on to what what you had said Austin this capability could negate like an entire wing in the air and space operations center, right. You have translators with intelligence professionals working through this type of data 24 hours a day, seven days a week. And those numbers are like 50 to 100 in a single AOC and then that scales to different locations throughout the theater. So huge use case and I see for sure. Yeah, and once you get it into text, then you can put all the text in one central location and then have one one big guy if looking at everything. Right. Yeah, it's really the I mean military operations in general is this idea of centralized command and decentralized execution. It's really that way to enable exactly that. And Greg has the coolest background I've ever seen for this question is about to ask. Great. I know you had talked to that Jeff said this summary that you were like just showing was like pretty good recap. Do you have an estimate on like how much it's missing like key information keeping track of that. Yeah, so from a transcription side, that's a great question. So just in law, Jay law put together really great analysis just on the transcription side of both word error rate and word information loss word information loss being the semantic loss that we got there. We aren't doing the same yet for the text side because I think there's two ways you can go do that. Either hand check or you can use a longer L on say, hey, does this summary actually match up with the text and get the diffs there. That's part of that model evaluation framework I was talking about that we're ADRing. And so we will have that data, but right now there's a little bit of eyeballing going on because even getting to the point where you're like feeling good about the evals takes a little bit of human in the loop, at least initially. Okay, for the second time, I want to get through some more stuff today to appreciate all of it, Jared, especially the rundown of kind of just everything that's changed not just the demo. I do think that's super helpful. Who did I think I had brand second on the list? I know brands around us all, it is super cool. He shed that he works in so brand over to you. All right, I'm going to get show in here what's going on. There's a lot of a lot of updates, a lot of things to change. We did the last products thing to talk about ADRs. What were we intending to go and accomplish? And so before we get all told out of wanting to highlight, often what you mentioned earlier, which was, hey, like we have a real Lula team now. And so this has been a long time coming. This has been something I've been very excited about, particularly because it's something that we derived from dash days. Dash days is the reason we had the space and time to go and experiment with Oscar and figure out what do we want to do and find out what do we not want to do. We've got, we had a lot of options and this is like a clear, I love Lula being one of the value pitches for why we do dash days. So that's such a big portion of this. Now we have a Lula team. Alvin Andy and Cole are, you know, really going to start to define what does this capable of? How are we going to implement it? Where is it going to be implemented and then greater mission priority? We have few one contract that we are doing development and implementation on as well as like obvious greater delivery system efforts. So I'm pretty excited. And so really what you see here right now is the current state. And this is a point time snapshot. I wanted to be very explicit about that, right? We have Lula on here has been sitting and I explicitly didn't change this because I want to highlight this Lula, the Kubernetes compliance engine. And it's been kind of how we've had it since, you know, to come last year. And I almost changed this and read me recently and I signed the whole off one because we're, we're emphasizing right now, like what we can do. And I think this is neat because the current refactor that I'm going to talk about is, you know, kind of reaching feature parity with what we had before, which was Kubernetes. But this and the rest of the, you know, project is always going to take leads and jumps through different iterations and improvements to the point where this headline has to change wholesale, where we don't want to, we don't want to, we don't need to, we can't be beholden to just Kubernetes for the compliance domain. We have to go and look at greater picture and in established context and greater capacity. So our AWS cloud infrastructure, generically, right? Awesome. Like there would be a lot of validation work performing those environments reaching out further, where, where else could we be validating information from? Is there key pieces of our tech stack that reveal, you know, kind of how they implement certain capabilities in different ways, which don't surface at the declarative level, ie, do we have to hit APIs? There's a lot to think about here. And so this is going to rapidly change as we start to figure out how this works and what is right. What are we doing? Why do we take the stances that we do is all of this story that I think we still need to tell. So in that, like the read me is slowly getting updated our docs are slowly getting built. Shout out to Finster for mentioning that he's going to help with that docs endeavor because I'm very excited about getting that up and running. But outside of that was changed. We did the ADR last time ADR kind of was outlining. Hey, we're going to start looking at some of these pieces of the puzzle. And what I mean by that is multiple provider support having Lula be able to kind of allow different providers to engage with the validation where applicable. Maybe one provider is very good at doing something to a one domain that I'll talk about where one is not in vice versa. And you want to use those appropriately. You want to use them. And so in order for us to start executing start getting us into the hands of operators and actually doing some stress testing on it. What does it look like? How does it work? This work got merged as of this morning, which was the first provider, OPA for those who want to talk about it. I'd love to talk about it about the decision process and why we went this route. But it was a clear definitive way of like looking at the constraints that we experienced previously and then finding out how to meld this into a concise statement of validation. Well, we've learned a lot of things that don't work over the last year. And I think that's super important. We want to continue to learn what doesn't work. So we can find out ways that do work. So the thing I wanted to really show today as part of that and just needs to get blown way up is one really like what we see the payload look alike. Disregard the off-scale formats currently. So we're going to really reach into what off-scale native is. It's kind of a term being tossed around in the ecosystem right now for off-scale native tooling. I'm really kind of looking at a similar payload to this where we can start to define different ways to write the story and different ways to use different providers together, even in one single cohesive package or validation. So you see provider, OPA here, domain, Kubernetes, just setting the context. What are we about to try and go do? Where do we want to go and get things for? And the demo I'm going to show today is around this statement here. This is a simple example in the next probably coming products. I want to show more realistic and complex scenarios in a plan to do that, especially around big bang. And so what I want to highlight here, though, is a constraint that we quickly hit with caverno these resource rules. So what did you want to go and validate when you're performing that execution? And we were beholden in the previous implementation to you can kind of only validate one thing at a time. And it's hard to view multiple things at a time and relate them to one another. And so we started to head down that path of what does this look like? And so here you can see we're getting all pods and deployments in in a specific namespace. This can be abstracted. This can be all namespaces or non namespace objects. And a lot of every permutation in between. And you can kind of see we're just going to do a basic check. This is super simple. We're going to look at pods and it's pod looking for the food label. And we want the respective value to be bar, same for deployments. And so how do you perform all of that at the same time and then allow people to start injecting additional validations? Hey, how do these things meld together? What can we then do that provides us with rich validation as opposed to just that surface layer that you recombinally see with like admission control. And so what I'm going to highlight here real fast cluster on the right side, no validation test namespace as you saw previously. Here's that I'll stay on the left side that we were looking at. And so we have some demo resources staged in the repository. I can buy a namespace. Really fast. We have a validation namespace created. Awesome. That's going to house this validation for what we're performing today. And then, for instance, if I wanted to, I could go and we had some past fail scenarios. So we'll create a pod here really fast. And I don't have any deployments yet. So we'll skip that. What do you do? 40. Wow, condition. Live demo people. So here we have the pod running awesome. I want to go and validate that with what we have today. And so, Lula, Validate, and we're going to point it out this all scale that we're looking at right up here. And we start to see things applying against and then it gives you the number of resources that it retrieved. This is what we're referring to is kind of like our data source or our domain Kubernetes. And so our domain had this payload. We want to go and retrieve these resources. There's only one pot over there. And so now we've confirmed that is the case. Now we want to look through various other scenarios. Right. And so we can do the failure scenario. And get that up and running here. We perform the validation. We already have that available to us. Awesome. I didn't hard code this to say past. It's my joke every time I tell this. But you can see pass fail. Awesome. Those real. Those are really just defined right now by labels. This is a pretty basic scenario. But what I wanted to look at was when we start to cross resource. Validate and this one doesn't really highlight that too natively yet. We want to show actually comparing things and diving into the nitty-gritty details of those resources. But right now, the thing that I've really been trying to get around the constraint of in this space is that idea of being able to do all this at one time. Grab all the resources that I need and perform some rich validation. I wouldn't necessarily consider this as rich validation just yet. But you can start to see multiple things happening now applying against four resources. Why that is we have three pods and one deployment and the pod that deployment is validating successfully the pod is not. And so these things that we can start to key in on as related resources. That's good to have. Can we also grab different things my demo pod for my test pod are they supposed to be configured in some way network policy also to what if I want to ensure that everything is kind of locked down so it's only communicating with what it means. I grab a network policy and confirm that it can only talk to these two things based upon the ports that are exposed. So a lot of rich details to go into here things that I really want to export further and just want to highlight this today that as of as emerging that first provider this morning. We're really starting to head down the path of what's find out what doesn't work because there are so many constraints in the validation and compliance space that I personally do not believe that your admission controller, you know, type validation fully encompasses it definitely encompasses like the passing things out that are against your policy. But getting to the point for what does that fully reconciled state really mean from the validation and compliance perspective. So open the floor for questions. There's a there's a whole lot. I'm sure that we could dive into if we wanted to, but try to keep it brief. Mads, do you still have your hand up from last time or do you have another question. I just want to maybe take a chance to brand appreciate the demo and the discussion. Take a chance to plug two things. One, if you haven't heard, I know we all have our heads down below the number of different they know communication silos it to some degree here. There's an open ATO event that's happening November 15th in Boston. It's in person around really thought leadership and community interested open source tooling of all things. ATO is it the 14th of the 15th because I just sent out a bunch of invites saying it was the 15th. Great. I guess we'll correct that. I didn't realize it changed the day before regardless. I'll figure it out. But did want to say if you know anybody that lives in like the sky cyber community Navy doesn't matter cybercom invite people get them there. It's really a discussion. It's actually centered around Brian Kropa who's actually an authorization official in the Air Force as a guppy. Kind of I don't know if we can say he's co hosting it with us that but I know he's going to be there all day and he's the keynote speaker and he's excited about all that so he's also the AO or several domains we work in on the delivery side. And then besides that Rob pointed out something great the other day to me that you know when we talk we think about our three unique really as a company, but especially on the product side, one of them being like this idea of security with compliance, but also being portable and open source. We really need this stuff to work out whether it's the however we do the compliance side of the world or fulfill that requirement. I think what branch leading is a lot of innovation this space. We have to be able to solve and communicate this well. Otherwise like the cloud service providers, Microsoft, I think Lucas just linked something the other day what Microsoft's up to in this same area, but there's people all over in this area that all sound very similar to what we're doing. But I still believe not all of them are doing the three things we are trying to do to make up the three uniques right all three together, but this is definitely the one where I think like really sets us apart. And the one that you know we're doubling down as you see investment wise. When it comes to resourcing here so any other questions thoughts before we move on to the manifests tool. Alright, I'm going to go update all the emails I sent that said the 14th or the 15th to the 14th now while case talks. Hey everyone, so the manifests tool just got version 0.14.2 released and basically what this entails is support for running the wasm modules and the admission controller phases. So essentially this was our main like task right now is we're really focused on supporting our own internal product teams and our mission heroes. And so basically these asks are going to get like super high priority for us and we're going to start working on them immediately. This wasm basically feature is a request from the k8s tool because effectively they're trying to where they want to replicate the the k8s tool agent, but also a the manifests tool so there will be a the manifests tool the k8s tool agent that's just like the the k8s tool package where you can pick which the k8s tool agent with the custom and Packers that we want. This latest feature basically adds this included files into the package.json and whatever these included files are it's going to actually include them in the controller image basically effectively building a custom controller image. A custom doctor container that employees and then therefore your your wasm files and the files that are necessary to run your wasm files are adjacent to your capabilities. So therefore you can easily make calls and into wasm call into your wasm functions and basically accomplish anything you need by like that. Other than that what we're looking at right now the think we're very focused on is bear it is very deep into the module testing. So when we do mpx the manifests tool in it and we create a new module so let's just say we're going to create I don't know the the k8s tool the manifests tool agent module. How do we test that right and what are our defaults for testing so it's basically just looking at the DEVACs looking at the very best ways to test stuff. Buster is actually helping us and you actually submitted a PR today on the docs website which is basically going to be a lot more templated a lot better. This is stuff in the right direction for us because we have our docs currently in markdown. And kind of like the message we're trying to get our products out there we want people to know about our products and this is a great step in the right direction. Also speaking of the k8s tool the k8s tool has a secondary request and that's basically an on schedule mechanism to the food API so they want the ability to run a code block of code on an interval and basically keep track of that because currently they're basically having to maintain some go code adjacent to some the manifests tool codes types of code and they would definitely like to consolidate that. So we just put in the ADR for that to decide how we're going to do it. And I took a lot of time like going through it and we were talking with everyone checking with the k8s tool team to. And then lastly we're looking at securing the service accounts cluster role for Kepper and actually during the the manifests tool office hours we got into like a two and a half hour conversation yesterday it was really good. And we got to talk with John and a lot of the people from our community and from our delivery team so it was great to hear their feedback. And basically we're effectively going to have two ways right so they're going to there's going to be a permissive mode for the manifests tool service account which is basically like pick the tires do whatever you want and then there's going to be a mode where basically looks at your capabilities looks at the objects that these capabilities are operating on and it actually constructs a cluster role based on this. And that's basically a working progress we were about to merge it yesterday but then we realized there's one thing that we hadn't counted or just about it and stuff. Other than that there's been a lot of a rate updates to Kubernetes fluent API which is essentially part of the manifests tool but it says own project that that's basically how the manifests tool calls other objects creates pods create config maps create virtual services and just been on that and done amazing work. And so yeah reach out to us all I was actually chatting with Carly today we were talking about like her native article and basically the message from the manifests tool to everyone is like come reach out to us let us know your pain points let us know how we can help there's something that you all need you all are trying to make modules for something currently there is not a functionality in the module let us know we can we can prioritize it for you. And that's it for me. Case when is the the manifests tool office hours I don't move to the new yeah. the manifests tool office hours is Wednesdays at 4 p.m. I believe. You're Eastern time right. Question for case before we move on. All right thanks case many mind you on I saw you in the participants list I hope you're in your Tesla or a Qt or wherever you are today. Yeah so I did something really stupid. I pulled on somebody's PR that really wasn't with Mac at all. So thanks Zach for telling me that that was at AMD images in that bundle so that's cool. But I think it's all so let me see here. You're using that on a Mac okay well just get me a cluster error because it was trying to deploy my arm side of the AMD said. So that's not going to work sir but that's fine. Let me see here yeah so there's a bunch of things going on a bunch of changes and obviously also stalling as I look through whatever changed in this PR. But the the CLDR is now just go ahead and deploy it back if I did because I wasn't totally convinced that this was going to work and I pulled it down. See here. Oh that's really sad. Why is that the case. Okay that's that's really cool. So maybe I've never ever called this once we are done again before a meeting. I got to say it manually because this is really actually painful at this point. So the okay that's really jacked. I'm sorry. This thing is like super jacked at this point. Okay so I'll just talk through some stuff because my entire stack is now broken so we'll have words later. Anyway the so one of the things that I was focusing on that I can't show you now because it's all entirely destroyed for some reason was. We're trying to make this like push button deployment for K3D so you can you know just automatically roll out a baseline deployment of that. And I'm going to get a look again to see what the craft this thing built. But the idea was you have this single place to actually deploy. And oh wow. Yeah that's that's great. Okay so you have the single place you can actually deploy and run everything with. And this actually is interesting that this is our bug. I'm not running into now that I've been talking to them about. The is our trisable key context from things before it actually to verify things. And so that is actually breaking things in a different way now. So let me. I'm going to change key context real quick to before I share my screen here. Yeah maybe something will work for me will be awesome. Okay I'll try to show a screen now and see how bad this is. On the bright side of my wide screen right now so at least you can see my screen maybe. Okay let's see here. I think maybe is our phone choking me this time. Yes deploy. Yeah so I literally just pulled down like five minutes ago. I'm a push that just completely works all the things so I'm trying to undo that. But because of a dark bug you can't have this magic Webster architectures. That because I check for them when it shouldn't check for them which is kind of cool. Okay so we'll see if that actually works. That's very good. Yeah so I. If anybody is actively for the call I just do my key context again to get over on that bug because it defaulted back to a different questions architecture when I tried to. Runs are. That's the same bug that I've been. Completed for a couple of days. So we welcome community PRs. Yeah well you know. We'll talk later. So it's a really good thing to do is create this like push button experience for K 3D. That's the same across the board. And so one of the things that we're working through here is like before we get into rewriting umbrella and all those other things. Like we wanted to have a baseline with standard for K 3D. And it's one of the things we're looking for is first we chose K 3D because we're focusing on K 3S as. So we didn't want to have the service match. We have to change how it's configured. Just to accommodate our dev or CI. So the idea was like you wouldn't touch any configuration at all and Istio for example. And then that would just automatically give you what you needed for your configuration. So so that was the idea. And so what we we did here for that is it's it's pretty basic on the K 3D side. You deploy a standard very normal K 3D thing, right? So we're just able to the service load balancer, which is clipper. And then we're body specific for this is really just for you know for helping with the using this remotely. Because I was under the question is how do you use this when you're like in an EC2 instance and you want to do it for local. And so there's a little thing here to tell you how to do that. And then things like machine ID don't work on back. So we just do it the fake one because it doesn't actually matter. We don't use it properly anyway in our K 3D volume out. So we just make a fake one. And then the most interesting thing here is we have this. There's this problem where Istio and our configurations unlike the docs. We use a more complex scenario than what the docs talk about in Istio. So we use a multi gateway strategy, which means we have different load balancer addresses we need to deal with. That doesn't really work with port 40 directly without multiple of imports, which goes back to the version of problem if you don't want to. Change your your port bindings. So or changes deals configuration. So what we're doing here is we bring the port bindings in to these weird high ports. We deploy them all be to handle. You know the normal stuff in that will be does to give us allocations. We're using the which I know this is actually supposed to be. That we're using the allocations here or the IP address from the node to get a node pull up to it. We're then making that into a release thing. This should look pretty stupid to you if you've read them, but all these stuff and get pretty only differences. We're not even doctor in spec. We're using. Keeps you tell them step. And then the unique thing is we have this a boxy thing. And what this does is this takes all of your. Your gateways, right? Your routes. And they're currently in one port. It takes and prints, but puts them back out to different gateway in points. So the result is if I curl my local host, it's actually going through local host to docker network, docker network to. Forty into the a j proxy, a j proxy, then 40 into the right host IP on the right node. For it to actually produce down to the right gateway, the right input gateway down to the right. Virtual service down to the right pod. So what what makes this very interesting is this means we can do. No weird DNF stuff. No Etsy hosts have hacking. No TLS junk. It just all works for us. And so right now, I just deployed this bundle. This bundle does not have the manifests tool. I'm going to do the manifests tool as dev mode this time just to show it to this easy to read. But you can also, you know, deploy the the manifests tool piece as well. This particular bundle to all the one that didn't have this released. But all we're doing is we're installing K3D. We're just running at first. I'm skipping in it. I'm doing yellow mode right now just for speed, but it's the same principle in the way. Metal will be as we just looked at. And then the STO one is. We have the vector as well, but I'll just show STO for right now. STO is using upstream. Only. So this is using the the official home way of installing it the way that big bang currently does it through the STO operator is. It's not deprecated. It's just no longer going to get future updates or future features, I guess. So it's like on the road to deprecation, but not deprecated yet. But this is the official way to do it now. We do bring a couple things in from big bang. So if you look here at these manifests, you'll notice these are very familiar to you. You know, this is the one that. You just know these are road physical security stuff. One's a bug in envoy. The correct indication stuff is from big bang. As you can see, they're in reference. So we're bringing in some pieces in. If you could figure out your big bang, then we're doing our gateways. This is the new gateway pattern. They've deprecated the old chart. They multi gateway charts. So it's like one chart per. But if you notice, there's no is our variables here. And that was kind of intentional. What we wanted to do was expose out just the little package on the upstream side and then allow you to just override the values you need or the values files in YouTube. And we did this. This custom little mini chart, right, which is well actually how we do the gateway configuration. If you provide those values. And you can see like what some of those are quite. So you can do pastor or you can do, you know, actual your own issued secrets. And you can see what those values look like right now. We're here. And so the result of that is over here on this side. So we have the ingress gateways. Metal will be. If you look at the gateways, you can see, you know, start admin at delivery system.dev. Pastor is just key code right now. We only had one host. We can always add more later. The tenant gateways are started at delivery system.dev. So okay. Cool. Great. That's wonderful. Whatever. So now we've deployed Istio from the upstream. We could use our ambiguous as well here. We're using the upstream right now. And it's kind of just pushed by the deployment. So I'm going to go on and do the manifests tooled dev using this capability that I didn't actually use the Istio capability that's published right now. I just threw it together last night because I wanted to experiment with how fast it would be to build one out. So I will show you that real quick too. This is. This is the capability. It's very small. We go and we. We're, you know, just you know, vertical service. We're calling it. There's a couple of creation things here. But basically we were saying when a service is created or updated with the label, you know, this label here, that gateway label, then we're going to watch it. We're going to try to validate it. And then we're going to try to make a gateway out of it. So host the photo information because there's different ways you can do that. Great. On a reference. And then apply that. And so what this ends up doing is when services are created with these labels. It will go and create this new virtual service and it will self update. And then when you delete the service, it will delete the virtual service as well. So. See here. I'm going to go back to. Here. Okay. So. The files are the whole idea. I'm just going to go. Pull that up real quick. It's just this simple. It's you've been file. So you have service count. Deployment. Pretty basic stuff. Nothing super fancy here. Happening. And this one. Actually, we're not addressing the issue injection label in this particular version. We could. But I think it's something we need to talk about. It might be better as a validation. We have hooked in an actual watch thing. But as you can see from the service definition, there is no virtual service. We have these annotations here. For a tenant. And then the host. We can also do port. But they will look for ht port. By default. If you don't include a port. So I'm going to first do the manifests tool dev. And again, it's just it's just far more readable this way than it is. Actually. Because when you run it in the cluster, it's using the. The JSON output, which is kind of messy to read. So what this finished putting up. And then you should see those. You want to see pods here because they're running locally. But you would see the. The mutation. And much of the toy stuff findings. Over there on the other side. So this is running now. So we can go back over here. And you see to apply. Our test step. I'll bring this a little bit. Okay, so those are created. And so you see this virtual service. I was just created here. And we can look at this. And we're going to do auto. Refresh on this, which is a very handy. GANS feature. So as you can see, we have demo.delivery system.dev. And you can see that's running there. This is going through the whole stack of things. We did without any configuration, right? Now if I wanted to change this. Let's call this thing. Demote to you because I'm very original. Okay, so you're going to see this thing. We'll update in a second. To demo to. There it is. And if I go back here and try to load this now. It's 404. And if I go back here and try to load this now. It's 404. Because now the binding is different. Right now on demo too. The same thing you could change gateways, right? You could. You could leave this as demo and. Change the gateway if you wanted to add. Then, for example. But this is the idea of kind of this rolling automatic update. You don't have to configure it everywhere. You can configure it one place. And then when I'm QCTO. Delete. This thing. That should also delete the virtual service. Because it's got a reference back to that. So we need to do anything that that's automatically. It's done for the finalizer. Clean it process. So now. We have this kind of like synchronized way to do this across the cluster. Now you might be thinking. What if I can control labels and for that. My intention is to take the capability. And use the settings that code. But do a watch. I'm probably can think back with the particular label that says. You know, I don't know. Okay, we can figure servers out of the some. We'll think of something. But basically it's the same thing. Except. We'll watch for us. Particularly config back with a particular thing. What's in the note here is this. This label. Is what's allowed us to be more efficient because under the hood. The. I think case mentioned it. Key cloak. I'm sorry. the manifests tool. Is calling the credit fluid client. Which has a watch with a. A label selector on it. So it's actually looking for these. The set based. They were selected to exist. So like this is only going to be watching for services. Right now on any namespace. With this exact label. Key on it. So it won't fire for anything else but those things. And it will. If it misses something because it's dead. It'll fire after we back up. So this will also get background jobs. So it's like. Let's see. You just in out of band. It would pick that up to when you started this back up. If this were dead. One last thing. We have the new vector stuff. It's working. It's working. It's working. It's working. It's working. One last thing. We have the new vector stuff as well. I want to go through that right now. But it's. We're working through the. This as well. This one is using a bit more. Is that correct? I think this is actually using. Something from iron bank more than we were using on our side. Right. For. New vector. It's. Yeah. It's using some of the. Customizations from. From Big Bang. But it's not using any of the iron bank stuff. It's using the upstream images and the upstream charts. Okay. Got you. Interesting. Okay. I don't know why it blew up on me with the. Amdc support stuff. And that's kind of strange. Okay. Yeah. So we have new vectors as well. I kind of work on this for. We're still going to like the process and the structure. But the idea here is to get a very fast additive push button. K3 that works for local dev for Mac and Linux and Windows. And for. CI. I think we've now accomplished that. And it's a zero complete because it. The TLS search. They're one year search. So they're not going to expire. Every five minutes. And then we also were doing. The correct multi gateway strategy. With the different wild cards. And you can. You can see those as well. I need to do it. And if you look at this certificate. You will see. That it is a secure. That was difficult. Let's start out delivery system. I have in this case. The expires. November of next year. So this is all like just you just get us a free. We're still have so tomorrow to finish this up. And show this completely with all those are non yolo mode stuff. For example, with all the images, which actually. Zack's further head than me on the image stuff. Because he has a load in already. But. This is an experiment right now to see what it would look like. With that umbrella. And without. The flux. And all other pieces. What we're finding is. It's actually not that bad. There's a big thing isn't doing as much as I thought it was doing. Questions. Jeff, I probably failed to set the stage a little bit on a little bit of a why. Because I think there's been a lot of rumours spread. I guess it's not even really rumours because it is true. About removing away from umbrella and moving away from flux when it comes to. To dubbed. And I'm going to do my best to articulate why I think we're doing that. And interested in your thoughts or voiceover of other people have opinions too. But. One is the company has to remember that we don't sell. Software because we're an open source company. We sell support. In order for us to provide good support as you start writing that out. And kind of the legal ease that you have to to convince people to actually give you money. Therefore pay everybody to keep working on open source things the community can benefit from. We have to do support really well. And when we have a dependency on platform one. It's not because we don't dislike platform one or we're going to move away from everything that they do. But if they fall behind in supports from the different things they do. The different abstractions between iron bank and big bang in their own ability to test and patch. It affects our ability to then tell customers that what we're paying us for is for those exact same things. So we can we basically set an artificial limit on our ability to support. On top of whatever platform one was already providing. Which kind of takes away from the entire business strategy of being an open source company. And having some way to monetize. So from the business reasons why is that's that's the major driver. But I do think there's other things that it should benefit everybody with one is like this. I'll call it like modularity for heroes like we have to be able to provide things in a modular fashion. Every delivery customer wants like a 20% delta from what we want to be the gold standard. We hope that that's not true over time and that people reduce their opinionation. But if we kind of have to meet customers where they are. And so setting that up this way allows us to be more modular than what umbrella provides out of the gate. And I think that's a huge deal as well. There's also this idea we have to integrate everything in this manual process. And a lot of times that's not us that's somebody else like an MTSI or north of Grumman or pick your favorite other company in our area. the manifests tool should give us a way to that having that integration be automatic by default now coming out of the box. And then finally, I think it sets us up for where we want to go on the road map right having a different user experience, which is really enabled by tighter integration all the way up the stack. And until we do those things, we can't really set ourselves up to provide that better developer and user or platform engineer experience until we do this kind of from the ground up. What I think you also hit on a little bit Jeff that may have gotten gloss over, I think you did the bottom line at the end and said the bottom line up front, which is my fault. I probably should have given some context first, but I think a lot of what you spent this time on was Zach and others in addition to the Istio new vector work with umbrella flux is this idea of standardizing around K3 in our own internal CI, which really impacts our dev cycles, right? I think that's been a consistent complaint that a lot of people for good reason it takes like an hour or two sometimes to run certain things and get feedback, especially when we're using EKS. Anyway, standardizing on K3D knowing that we're going to do an RKA to production deployments and kind of bringing K8 with us where we go, which seems counterintuitive if you're going to cloud service provider, but make sense for us in our uniques. All those things are kind of combined into what you just showed from a workflow perspective. I'm going to stop there and see if you agree and if other people have questions or comments. Yeah, and I did read back to the comments and just to clarify, I'm sorry, Zach, that was not his fault. There was actually a the k8s tool bug that bit me again. It randomly picks a kid context whenever you don't have a valid one and that last one that picked most Azure, which was the end of 64. So that wasn't the k8s tool. The the k8s tool team ever will talk. I agree, generally speaking, I think the K3D move is important for us because people will still want to use RKA to, they'll still want to use other pieces. But it will allow us to standardize both the local dev and the test, but also I think we'll reduce the lag time and the feedback loop for that because you're going to be having a lot less resources, it'll boot a lot faster. I mean, just thinking about how long it takes you to get to fully Istio running on Big Bang right now, you know, you deploy the thing and wait 10 minutes. And now you have eventually flux and those pieces that didn't get to Istio in this reconciliation. And we just did it in two and a half minutes. And I think that's the difference is you're reducing a lot of stack complexity, but also you're unifying our K3D works across the different environments so that you're not changing the duration. The general issue is most people in the company don't know Istio. Period. Istio is a lot. It takes expertise and it's a bunch of crap and it's a bunch of crap that we shouldn't really need to know. But the most part, it's the biggest, most complex system we have easily by foreign the entire stack. It's just we use like 1% of it. And so by abstracting most of that complexity and unifying it and so we're not doing comparison drift between dev, test and prod, I think it'll really help alleviate pain. And then we can focus on the other areas of complexity higher in the stack. I got a quick question. So now that we're looking to go down this route of kind of breaking up the big bang. Does this mean that we're looking at obviously other places to get our images. Does this mean we're entertaining like chain guard for our images or what are our thoughts around that? Yeah, and obviously we we're tracking the chain guard piece and there is a they're pretty expensive for their SLA. I think Jared mentioned they're doing something with Apple belongs, which is something we played with a few months ago and the manifests tool as well. It's definitely a good option. I don't think there's a particular reason we wouldn't do that. It's more of a business reason. Like we don't want to totally screw chain guard because their investment resources are to so we need to find out like a good business. It's hard to ship there because we could definitely do like what we probably did across the board for all big bang. That would actually not be the biggest lift because it's mostly done by the published wolfie stuff. I just don't know if you want to do that to chain guard because I mean they have to make money too. Yeah, and actually to to be to also add on top of that we're not using the chain guard images repo that uses their terraform. So, for example, like we're not using their s bomb say toward like verification right so. In this case, we're literally just using upco and and wolfie Casey. So like there's there's obviously a lot of value in chain guard the brand name as far as like what you go and validate against right. Yeah, I was going to hand up so. Yeah, I just want to see. It's a high level still right now, but where do you see the future of the big bang extension in the k8s tool now that we're kind of breaking up. Now they're kind of breaking up umbrella. Yeah, that's a hot topic. I think big picture. Thanks for asking that question. I guess I deserve that one. The big bang extension. I personally. Unless the affinity just absolutely begs for it. I don't know that it's a long term thing for us. It was like a square peg round hole that we sort of forced in because big bang was a nightmare and actions, you know, are needed for that work. And then the photo structure stuff wasn't enough. So we wanted to like pull something out for what that's what we call the extensions and so like it definitely was. It was like a very rudimentary step. And a tip to the right direction. I think this is a far more. What I call the k8s tool native approach. And so long term, I expect within six months. You know, we have fully ported everything off of umbrella. Everything off of the the flux structure. And it's all all of delivery system is using this new pattern. And we've established kind of the dorms around how you want to break things up and abstract things. Because you know, if it's our variables versus how values, for example. And then the dubbed or the extension, doing extension that dubbed uses. Because the thing that we ask community about and maybe it ends up being something similar to the way that we had to do the k8s tool UI. Where it has to go spin off thing. That's actually where my head's at. That thing that's a that's a the k8s tool team opens with community discussion though. Maybe with also with growth and even company leadership because. There is this thing that we're doing if we do that perception wise where we're like. Use our stuff or or suck it, which is not totally healthy for the open source side either. So I think it's like a. I don't want the team to have to maintain it personally. I don't think it's fair to ask the k8s tool team to maintain that. I think it never what's fair for the k8s tool that the maintain the vivid extension. That was never really the the long term goal. But that's what we're at today. I would rather just have that move out completely. I just don't know if the community can let us that easily. Cover that. Any other last comments and over a few minutes over. Want to be respectful people time. No several people dropped off and should have other commitments. Appreciate the time and energy and effort appreciate people presenting and going through stuff. And Zach for breaking your bill that brought me great pleasure. So the might means are actually, but yes, it was funny. All right, appreciate everybody. See you again in two weeks in this form. Thanks. Thanks everyone. You You You